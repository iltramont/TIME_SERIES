---
title: "Hungarian Chickenpox Analysis"
output: html_notebook
---

In this notebook there will be an analysis of Hungarian weekly chickenpox cases. First, let's have a look at the data.

# EDA

```{r}
# Load data
data = read.csv('hungary_chickenpox.csv')
print(paste('data is a', class(data)))
```
```{r}
print(data)
```
Is possible to see how the dataset contains 522 rows (weeks) and 21 columns (1 for the date and 20 cities).
Now we must convert the first column in the proper format and transform the dataset in a timeseries.
```{r}
data$Date <- as.Date(data$Date, format='%d/%m/%Y')
print(str(data))
```
```{r}
library(zoo)
zoo_data <- zoo(data[2:ncol(data)], order.by=data$Date)
#print(str(zoo_data))
plot.zoo(zoo_data, plot.type='multiple', nc=2, xlab = 'Date',
         main='Hungarian Chickenpox Weekly Cases', yax.flip = TRUE)
```
A seasonal behavior is clear in every city. It seems also that the peaks coincide.Let's check. For a better comparison, let's scale the data in order to eliminate differences in population between cities.

```{r}
# To have 20 different colors
library(RColorBrewer)
colors <- c(brewer.pal(n=12, name='Set3'), brewer.pal(n=8, name='Dark2'))
# Create a function to scale each column between 0 and 1
min_max_scaler <- function(x){
  (x - min(x)) / (max(x) - min(x))
}
data_scaled <- as.data.frame(lapply(data[, -1], min_max_scaler))
zoo_scaled <- zoo(x = data_scaled, order.by=data$Date)
plot.zoo(zoo_scaled, plot.type='single', xlab = 'Date',
         main='Hungarian Chickenpox Weekly Cases SCALED', col=colors)
```
Let's now compute the correlation matrix.
```{r}
corr_matrix <- cor(zoo_data, method = 'pearson')
library(corrplot)
corrplot(corr_matrix, method = 'square', type='full', addCoef.col = 'black',
         number.cex = 0.5, diag = FALSE, number.digits = 1, tl.cex=0.5, tl.col = 'black')
print(paste('Minimum correlation value is:', min(corr_matrix)))
```
As expected, the correlation is very high between all the cities (always higher than 0.25).

Now, Let's focus on the country aggregate.
```{r}
total_cases <- rowSums(data[, -1])
total_cases <- zoo(total_cases, order.by = data$Date)
plot.zoo(total_cases, main = 'Hungarian Chickenpox Cases (Reported)',
         ylab = 'Count', xlab = 'Date')
```
It is clear a seasonal pattern (more cases in cold months?) and is also possible to notice that peaks' height is decreasing. To reduce oscillations, a monthly aggregation is now performed. There will be used a "sum" aggregation function as datasets refers to reported cases and not active cases.
```{r}
monthly_data <- aggregate(zoo_data, as.yearmon, sum)
monthly_total <- aggregate(total_cases, as.yearmon, sum)
total_ts <- ts(monthly_total, start = c(2005, 1), freq=12)
print(total_ts)
plot(monthly_data, yax.flip = TRUE, main = 'Monthly Reported Cases', xlab = 'Date')
plot(monthly_total, main = 'Monthly Reported Cases', ylab = '', xlab = 'Date')
boxplot(total_ts ~ cycle(total_ts), xlab = 'Months', ylab = 'Cases',
        main='Monthly Cases Distribution')
```
As expected, the number of reported cases is higher in cold month that are also months in which schools are open and virus can spread easily.

# Decomposition

First, we will try with the additive method.
```{r}
cases_decompose <- decompose(total_ts)
plot(cases_decompose)
par(mar = c(5, 4, 6, 2))
plot(acf(c(cases_decompose$random), na.action = na.pass, plot = FALSE),
     main = 'Residuals ACF (Additive)')
```
The downward trend is clear. Is remarkable that the random component's volatility is lower when cases are low; this suggest to use the multiplicative decomposition. We see also a significant autocorrelation when the lag is 4 and 5.
```{r}
cases_decompose <- decompose(total_ts, type = 'multiplicative')
plot(cases_decompose)
par(mar = c(5, 4, 6, 2))
plot(acf(c(cases_decompose$random), na.action = na.pass, plot = FALSE),
     main = 'Residuals ACF (Multiplicative)')
```
With the multiplicative approach, the volatility of the random component is more constant and the autocorrelation in the random component is less significant (but we still have a significant autocorrelation with lag = 4).

# Forecasting
## Holt-Winters Forecast
```{r}
library(forecast)
# Fit additive and multiplicative models
hw_monthly_mul <- hw(monthly_total, seasonal='multiplicative')
hw_monthly_add <- hw(monthly_total, seasonal='additive')
# Compute sum of squared residuals
ssr_mul <- sum((hw_monthly_mul$fitted - hw_monthly_mul$x)^2)
ssr_add <- sum((hw_monthly_add$fitted - hw_monthly_add$x)^2)
print(paste('SSR multiplicative: ', ssr_mul))
print(paste('SSR additive: ', ssr_add))
# Keep the best model according to SSR (multiplicative model)
# Compute 24 months forecast horizon
forecast_hw <- forecast(hw_monthly_mul, h=24)
plot(forecast_hw)
lines(hw_monthly_mul$fitted, col='blue')
legend('topright', legend = c('observed', 'fitted', 'forecast'), col = c('black', 'blue', 'cyan'), lwd = 2, cex = 0.7)
print(paste('Root mean squared error on fitted values: ', sqrt(ssr_mul / length(hw_monthly_mul$residuals))))
```
## ARIMA Forecast
```{r}
# Use auto.arima function
arima_model <- auto.arima(total_ts)
# Compute sum of squared residuals
ssr_arima <- sum((arima_model$fitted - arima_model$x)^2)
print(paste('SSR arima: ', ssr_arima))
print(paste('Root mean squared error on fitted values: ', sqrt(ssr_arima / length(arima_model$residuals))))

print(arima_model)
forecast_arima <- forecast(arima_model, h=24)
plot(forecast_arima, main = 'Monthly Reported Cases ARIMA')
points(forecast_arima$fitted, col='blue', type='l', lty=2)
legend('topright', legend = c('observed', 'fitted', 'forecast'), col = c('black', 'blue', 'cyan'), lwd = 2, cex = 0.7)
```
# Plot arima and HW predictions in the same plot
```{r}
fitted_df <- cbind(total_ts, hw_monthly_mul$fitted, arima_model$fitted)
colnames(fitted_df) <- c('Original', 'HW_fitted', 'ARIMA_fitted')
plot(fitted_df, plot.type = 'single', col = c('black', 'blue', 'red'))
legend('topright', legend = c('Original', 'HW_fitted', 'ARIMA_fitted'), col = c('black', 'blue', 'red'), lwd = 2, cex = 0.7)
```
# Plot forecasts in the same plot
```{r}
forecast_df <- matrix(nrow=(length(total_ts) + length(forecast_arima$mean)), ncol=3)
forecast_df[1:length(total_ts), 1] <- total_ts
forecast_df[(length(total_ts)+1): nrow(forecast_df), 2] <- as.numeric(forecast_hw$mean)
forecast_df[(length(total_ts)+1): nrow(forecast_df), 3] <- forecast_arima$mean
colnames(forecast_df) <- c('Original', 'Forecast HW', 'Forecast ARIMA')
forecast_df <- ts(forecast_df, start = c(2005, 1), freq=12 )
plot(forecast_df, plot.type='single', col=c('black', 'blue', 'red'),
     main = 'Forecast: HW vs ARIMA')
legend('topright', legend = c('Original', 'HW_forecast', 'ARIMA_forecast'), col = c('black', 'blue', 'red'), lwd = 2, cex = 0.7)
```


# Model Evaluation
```{r}
# Create functions to compute metrics
mse <- function(y_true, y_pred){
  return(mean((y_pred - y_true)^2))
}
me <- function(y_true, y_pred){
  return(mean(y_pred - y_true))
}
mad <- function(y_true, y_pred){
  return(mean(abs(y_pred - y_true)))
}
rmse <- function(y_true, y_pred){
  return(sqrt(mean((y_pred - y_true)^2)))
}
mape <- function(y_true, y_pred){
  return(mean(abs((y_pred - y_true) / y_true)))
}

mpe <- function(y_true, y_pred){
  return(mean((y_pred - y_true) / y_true))
}

compute_metrics <- function(x, y){
  return(c(mse(x, y), me(x, y), mad(x, y), rmse(x, y), mape(x, y), mpe(x, y)))
}
hw_mul_metrics <- compute_metrics(total_ts, hw_monthly_mul$fitted)
hw_add_metrics <- compute_metrics(total_ts, hw_monthly_add$fitted)
arima_metrics <- compute_metrics(total_ts, arima_model$fitted)
metric_name <- c('MSE', 'ME', 'MAD', 'RMSE', 'MAPE', 'MPE')

metrics_df <- as.data.frame(cbind(hw_mul_metrics, hw_add_metrics, arima_metrics))
metrics_df <- format(round(metrics_df, 2), scientific=FALSE)
row.names(metrics_df) <- metric_name
print(metrics_df)
```
# Check for normality of residuals
```{r}
hw_mul_res <- hw_monthly_mul$fitted - total_ts
hw_add_res <- hw_monthly_add$fitted - total_ts
arima_res <- arima_model$fitted - total_ts

# Histograms and QQ-plot

# HW multiplicative
res <- hw_mul_res
hist(res, prob=TRUE, main = 'Histogram of HW multiplicative residuals')
# Add gaussian line to plot
mu <- mean(res)
sigma <- sd(res)
x <- seq(min(res), max(res), length=80)
y <- dnorm(x, mu, sigma)
# Add line
lines(x, y, col='red')
qqnorm(res, main = 'QQ-plot HW multiplicative model')
qqline(res)


# HW additive
res <- hw_add_res
hist(res, prob=TRUE, main = 'Histogram of HW additive residuals')
# Add gaussian line to plot
mu <- mean(res)
sigma <- sd(res)
x <- seq(min(res), max(res), length=80)
y <- dnorm(x, mu, sigma)
# Add line
lines(x, y, col='red')
qqnorm(res, main = 'QQ-plot HW additive model')
qqline(res)

# ARIMA
res <- arima_res
hist(res, prob=TRUE, main = 'Histogram of ARIMA residuals')
# Add gaussian line to plot
mu <- mean(res)
sigma <- sd(res)
x <- seq(min(res), max(res), length=80)
y <- dnorm(x, mu, sigma)
# Add line
lines(x, y, col='red')
qqnorm(res, main = 'QQ-plot ARIMA model')
qqline(res)
```
```{r}
# Jarque-brera normality test
library(tseries)
print(paste('p-value for multiplicative HW:', jarque.bera.test(hw_mul_res)$p.value))
print(paste('p-value for additive HW:', jarque.bera.test(hw_add_res)$p.value))
print(paste('p-value for ARIMA:', jarque.bera.test(arima_res)$p.value))
```
With arima model, residuals on fitted values are not normally distributed (p-value of Jarque-Brera test is less than 5%).

# Time Frequency Analysis

# ARCH / GARCH

# VAR




























